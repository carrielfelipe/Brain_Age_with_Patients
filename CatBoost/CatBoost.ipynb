{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=  'C:/Users/felipe/Documents/Brain/'\n",
    "#import sys\n",
    "#sys.path.append(path) \n",
    "path_=  'C:/Users/felipe/Documents/Brain/Brain_Model/'\n",
    "import sys\n",
    "sys.path.append(path_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "from base_regressor import BaseRegressor\n",
    "from Plotter import Plotter\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "class CatBoostRegressor(BaseRegressor):\n",
    "    def __init__(self, save_path=None, scaler=None, params=None, params_space=None, fit_params_search=None,model_params_search=None,fit_params_train=None,model_params_train=None, name_model=\"CatBoost\"):\n",
    "        super().__init__(save_path, scaler, params, params_space, fit_params_search ,model_params_search,fit_params_train ,model_params_train, name_model)\n",
    "                \n",
    "        self.model_ml= cb.CatBoostRegressor\n",
    "        self.params = {\n",
    "            'loss_function': 'RMSE',  \n",
    "            'eval_metric': 'RMSE'  \n",
    "        }\n",
    "        \n",
    "        self.params_space = {\n",
    "            'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "            'depth': Integer(3, 10),\n",
    "            'n_estimators': Integer(50, 5000),\n",
    "            'l2_leaf_reg': Real(1, 20)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72 Features\n",
    "features = ['Alpha2_canon_EPP_HPC_left', 'Alpha2_canon_EPP_HPC_right', 'Alpha2_canon_EPP_PARIET_left', 'Alpha2_canon_EPP_PARIET_right', 'Alpha2_canon_EPP_OCC_left', 'Alpha2_canon_EPP_OCC_right', 'Alpha2_canon_RPD_HPC_left', 'Alpha2_canon_RPD_HPC_right', 'Alpha2_canon_RPD_CING_left', 'Alpha2_canon_RPD_CING_right', 'Alpha2_canon_RPD_PARIET_left', 'Alpha2_canon_RPD_PARIET_right', 'Alpha2_canon_RPD_OCC_left', 'Alpha2_canon_RPD_OCC_right', 'High_subj_spec_EPP_HPC_left', 'High_subj_spec_EPP_HPC_right', 'High_subj_spec_EPP_CING_left', 'High_subj_spec_EPP_CING_right', 'High_subj_spec_EPP_PARIET_left', 'High_subj_spec_EPP_PARIET_right', 'High_subj_spec_EPP_OCC_left', 'High_subj_spec_EPP_OCC_right', 'High_subj_spec_RPD_HPC_left', 'High_subj_spec_RPD_HPC_right', 'High_subj_spec_RPD_CING_left', 'High_subj_spec_RPD_CING_right', 'High_subj_spec_RPD_PARIET_left', 'High_subj_spec_RPD_PARIET_right', 'High_subj_spec_RPD_OCC_left', 'High_subj_spec_RPD_OCC_right', 'Low_subj_spec_EPP_HPC_left', 'Low_subj_spec_EPP_HPC_right', 'Low_subj_spec_EPP_CING_left', 'Low_subj_spec_EPP_CING_right', 'Low_subj_spec_EPP_PARIET_left', 'Low_subj_spec_EPP_PARIET_right', 'Low_subj_spec_EPP_OCC_left', 'Low_subj_spec_EPP_OCC_right', 'Low_subj_spec_RPD_HPC_left', 'Low_subj_spec_RPD_HPC_right', 'Low_subj_spec_RPD_CING_left', 'Low_subj_spec_RPD_CING_right', 'Low_subj_spec_RPD_PARIET_left', 'Low_subj_spec_RPD_PARIET_right', 'Low_subj_spec_RPD_OCC_left', 'Low_subj_spec_RPD_OCC_right', 'TF_ORB_left', 'TF_ORB_right', 'TF_IFG_left', 'TF_IFG_right', 'TF_MFG_left', 'TF_MFG_right', 'TF_SFG_left', 'TF_SFG_right', 'TF_HPC_left', 'TF_HPC_right', 'TF_INS_left', 'TF_INS_right', 'IAF_ORB_left', 'IAF_ORB_right', 'IAF_IFG_left', 'IAF_IFG_right', 'IAF_MFG_left', 'IAF_MFG_right', 'IAF_SFG_left', 'IAF_SFG_right', 'IAF_HPC_left', 'IAF_HPC_right', 'IAF_INS_left', 'IAF_INS_right', 'IAF_OCC_left', 'IAF_OCC_right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import pickle\n",
    "\n",
    "\n",
    "file_path_CN = f'{path}Brain_Aging/CN_freq.xlsx'\n",
    "file_path_AD = f'{path}Brain_Aging/AD_freq.xlsx'\n",
    "file_path_FTD = f'{path}Brain_Aging/FTD_freq.xlsx'\n",
    "file_path_MCI = f'{path}Brain_Aging/MCI_freq.xlsx'\n",
    "file_path_PD = f'{path}Brain_Aging/PD_freq.xlsx'\n",
    "\n",
    "df_CN = pd.read_excel(file_path_CN)\n",
    "df_AD = pd.read_excel(file_path_AD)\n",
    "df_FTD = pd.read_excel(file_path_FTD)\n",
    "df_MCI = pd.read_excel(file_path_MCI)\n",
    "df_PD = pd.read_excel(file_path_PD)\n",
    "\n",
    "df_CN_filtrado = df_CN[(df_CN['Age'] >=50) & (df_CN['Age'] <= 90)].reset_index(drop=True)\n",
    "df_AD_filtrado = df_AD[(df_AD['Age'] >= 50) & (df_AD['Age'] <= 90)].reset_index(drop=True)\n",
    "df_FTD_filtrado = df_FTD[(df_FTD['Age'] >= 50) & (df_FTD['Age'] <= 90)].reset_index(drop=True)\n",
    "df_MCI_filtrado = df_MCI[(df_MCI['Age'] >= 50) & (df_MCI['Age'] <= 90)].reset_index(drop=True)\n",
    "df_PD_filtrado = df_PD[(df_PD['Age'] >= 50) & (df_PD['Age'] <= 90)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "X_CN = df_CN_filtrado[features]\n",
    "y_CN = df_CN_filtrado[\"Age\"]\n",
    "ID_CN = df_CN_filtrado[\"ID-unique\"]\n",
    "\n",
    "X_AD = df_AD_filtrado[features]\n",
    "y_AD = df_AD_filtrado[\"Age\"]\n",
    "ID_AD = df_AD_filtrado[\"ID-unique\"]\n",
    "\n",
    "X_FTD = df_FTD_filtrado[features]\n",
    "y_FTD = df_FTD_filtrado[\"Age\"]\n",
    "ID_FTD = df_FTD_filtrado[\"ID-unique\"]\n",
    "\n",
    "X_MCI = df_MCI_filtrado[features]\n",
    "y_MCI = df_MCI_filtrado[\"Age\"]\n",
    "ID_MCI = df_MCI_filtrado[\"ID-unique\"]\n",
    "\n",
    "X_PD = df_PD_filtrado[features]\n",
    "y_PD = df_PD_filtrado[\"Age\"]\n",
    "ID_PD = df_PD_filtrado[\"ID-unique\"]\n",
    "\n",
    "# \n",
    "X_combined = pd.concat([X_CN, X_AD, X_FTD, X_MCI, X_PD], axis=0).reset_index(drop=True)\n",
    "#X_combined = pd.concat([X_CN, X_AD], axis=0).reset_index(drop=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_CN)\n",
    "\n",
    "X_CN_scaled = scaler.transform(X_CN)\n",
    "X_AD_scaled = scaler.transform(X_AD)\n",
    "X_FTD_scaled = scaler.transform(X_FTD)\n",
    "X_MCI_scaled = scaler.transform(X_MCI)\n",
    "X_PD_scaled = scaler.transform(X_PD)\n",
    "\n",
    "X_CN_scaled = pd.DataFrame(X_CN_scaled, columns=X_CN.columns)\n",
    "X_AD_scaled = pd.DataFrame(X_AD_scaled, columns=X_AD.columns)\n",
    "X_FTD_scaled = pd.DataFrame(X_FTD_scaled, columns=X_FTD.columns)\n",
    "X_MCI_scaled = pd.DataFrame(X_MCI_scaled, columns=X_MCI.columns)\n",
    "X_PD_scaled = pd.DataFrame(X_PD_scaled, columns=X_PD.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenado_CN = pd.concat([X_CN, y_CN, ID_CN], axis=1, ignore_index=False)\n",
    "df_concatenado_AD = pd.concat([X_AD, y_AD, ID_AD], axis=1, ignore_index=False)\n",
    "df_concatenado_FTD = pd.concat([X_FTD, y_FTD, ID_FTD], axis=1, ignore_index=False)\n",
    "df_concatenado_MCI = pd.concat([X_MCI, y_MCI, ID_MCI], axis=1, ignore_index=False)\n",
    "df_concatenado_PD = pd.concat([X_PD, y_PD, ID_PD], axis=1, ignore_index=False)\n",
    "\n",
    "lista_dfs = [df_concatenado_AD, df_concatenado_FTD, df_concatenado_MCI,df_concatenado_PD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor()\n",
    "Plotters = Plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'neg_mean_squared_error'\n",
    "#'neg_root_mean_squared_error'\n",
    "#'explained_variance'\n",
    "#'r2'\n",
    "\n",
    "opt_model, best_params = model.search_best_model (X=X_CN_scaled, y=y_CN, n_iter_=50, scoring_metric='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path}Brain_Aging/Opt_Model/opt_CatBoost.pickle', 'wb') as file:\n",
    "        pickle.dump(opt_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path}Brain_Aging/Opt_Model/opt_CatBoostpickle', 'rb') as file:\n",
    "    opt_model= pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = opt_model.cv_results_['mean_test_score'][:50]\n",
    "Plotters.plot_iteration(y=score, title='BayesSearch', xlabel='Iteration', ylabel='score', legend_result=False, best_result=True, y_size=2, font='Times New Roman', weight='normal', mode=2, band_width=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_ = model.best_hyper(num_best=10, opt_model=opt_model, num_max=50)\n",
    "best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_labels_df_CN_train, results_labels_df_CN_test, results_labels_list, results_model, results_per_fold_CN_train, results_per_fold_CN_test, results_per_fold_pat= model.trainer(df_concatenado_CN, lista_dfs, n_splits=10, n_iterations=20, params_=best_params_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results_training = [results_labels_df_CN_train, results_labels_df_CN_test, results_labels_list, \n",
    "           results_model, results_per_fold_CN_train, results_per_fold_CN_test, results_per_fold_pat]\n",
    "\n",
    "with open('results_model/resultados_modelo.pkl', 'wb') as f:\n",
    "    pickle.dump(results_training, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_model/resultados_modelo.pkl', 'rb') as f:\n",
    "    loaded_results = pickle.load(f)\n",
    "\n",
    "results_labels_df_CN_train, results_labels_df_CN_test, results_labels_list, results_model, results_per_fold_CN_train, results_per_fold_CN_test, results_per_fold_pat = loaded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_labels_df_AD=results_labels_list[0]\n",
    "results_labels_df_FTD=results_labels_list[1]\n",
    "results_labels_df_MCI=results_labels_list[2]\n",
    "results_labels_df_PD=results_labels_list[3]\n",
    "\n",
    "results_labels_df_total = [results_labels_df_CN_test, results_labels_df_AD, results_labels_df_FTD,results_labels_df_MCI,results_labels_df_PD]\n",
    "\n",
    "results_avg_list = model.avg_list(results_labels_df_total)\n",
    "\n",
    "results_avg_list_CN = results_avg_list[0]\n",
    "results_avg_list_AD = results_avg_list[1]\n",
    "results_avg_list_FTD = results_avg_list[2]\n",
    "results_avg_list_MCI = results_avg_list[3]\n",
    "results_avg_list_PD = results_avg_list[4]\n",
    "\n",
    "df_CN_seleccion = df_CN_filtrado[['ID-unique', 'Country_ID', 'Age', 'Sex', 'Educ', 'MMSE', 'HDI', 'GINI', 'ODQ']]\n",
    "df_CN_combined = results_avg_list_CN.merge(df_CN_seleccion, on='ID-unique', how='left')\n",
    "\n",
    "df_AD_seleccion = df_AD_filtrado[['ID-unique', 'Country_ID', 'Age', 'Sex', 'Educ', 'MMSE',  'GINI', 'ODQ']]\n",
    "df_AD_combined = results_avg_list_AD.merge(df_AD_seleccion, on='ID-unique', how='left')\n",
    "\n",
    "df_FTD_seleccion = df_FTD_filtrado[['ID-unique', 'Country_ID', 'Age', 'Sex', 'Educ', 'MMSE', 'GINI', 'ODQ']]\n",
    "df_FTD_combined = results_avg_list_CN.merge(df_FTD_seleccion, on='ID-unique', how='left')\n",
    "\n",
    "df_MCI_seleccion = df_MCI_filtrado[['ID-unique', 'Country_ID', 'Age', 'Sex', 'Educ', 'MMSE',  'GINI', 'ODQ']]\n",
    "df_MCI_combined = results_avg_list_MCI.merge(df_MCI_seleccion, on='ID-unique', how='left')\n",
    "\n",
    "df_PD_seleccion = df_PD_filtrado[['ID-unique', 'Country_ID', 'Age', 'Sex', 'Educ', 'MMSE',  'GINI', 'ODQ']]\n",
    "df_PD_combined = results_avg_list_PD.merge(df_PD_seleccion, on='ID-unique', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los dataframes combinados en archivos Excel\n",
    "df_CN_combined.to_excel('results/df_CN_combined.xlsx', index=False)\n",
    "df_AD_combined.to_excel('results/df_AD_combined.xlsx', index=False)\n",
    "df_FTD_combined.to_excel('results/df_FTD_combined.xlsx', index=False)\n",
    "df_MCI_combined.to_excel('results/df_MCI_combined.xlsx', index=False)\n",
    "df_PD_combined.to_excel('results/df_PD_combined.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame en formato largo para los errores\n",
    "corrected_gap_AD = pd.to_numeric(results_avg_list[1]['GAP_corrected'])\n",
    "corrected_gap_FTD = pd.to_numeric(results_avg_list[2]['GAP_corrected'])\n",
    "corrected_gap_MCI = pd.to_numeric(results_avg_list[3]['GAP_corrected'])\n",
    "corrected_gap_CN_test = pd.to_numeric(results_avg_list[0]['GAP_corrected'])\n",
    "corrected_gap_PD = pd.to_numeric(results_avg_list[4]['GAP_corrected'])\n",
    "\n",
    "data = {\n",
    "    'Error': np.concatenate([corrected_gap_PD, corrected_gap_AD, corrected_gap_FTD, corrected_gap_MCI, corrected_gap_CN_test]),\n",
    "    'Grupo': ['PD'] * len(corrected_gap_PD) + ['AD'] * len(corrected_gap_AD) + ['FTD'] * len(corrected_gap_FTD) + ['MCI'] * len(corrected_gap_MCI) + ['CN'] * len(corrected_gap_CN_test)\n",
    "}\n",
    "\n",
    "df_errors = pd.DataFrame(data)\n",
    "\n",
    "# Definir la paleta de colores personalizada\n",
    "colores_personalizados = {\n",
    "    'AD': 'skyblue',\n",
    "    'FTD': 'red',\n",
    "    'MCI': 'lightgreen',\n",
    "    'PD': 'orange',\n",
    "    'CN': 'gray'\n",
    "}\n",
    "\n",
    "Plotters.plot_gap_distribution(df_errors, colores_personalizados,x_size=6, y_size=3,font='Times New Roman', weight='normal', xlabel='Gap (years)', x_limits=(-3, 4),fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_avg_CN = results_avg_list[0][['y_labels', 'y_pred']].copy()\n",
    "results_avg_AD = results_avg_list[1][['y_labels', 'y_pred']].copy()\n",
    "results_avg_FTD     = results_avg_list[2][['y_labels', 'y_pred']].copy()\n",
    "results_avg_MCI = results_avg_list[3][['y_labels', 'y_pred']].copy()\n",
    "results_avg_PD = results_avg_list[4][['y_labels', 'y_pred']].copy()\n",
    "\n",
    "df_list = [results_avg_CN, results_avg_AD, results_avg_FTD, results_avg_MCI, results_avg_PD]\n",
    "colors = {'CN': 'black', 'AD': 'blue', 'FTD': 'red', 'MCI': 'green', 'PD': 'orange'}\n",
    "labels = ['CN', 'AD', 'FTD', 'MCI', 'PD']\n",
    "Plotters.plot_regression_diagnosis(df_list=df_list, colors=colors, title='Brain Age Comparison',labels=labels,x_ticks_step=10,y_ticks_step=2,x_min_limit=50,y_min_limit=62,x_max_limit=90,y_max_limit=72,line_ideal=False, alpha=0.5, xlabel='Chronological Age',ylabel='Brain Age',  font='Times New Roman', weight='normal', x_size=4,y_size=4, legend=True, fontsize=13, mode=3,xticks =1,yticks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def shap_values_calculate(df_train, df_test, results_per_fold_train, results_per_fold_test, results_model, feature_col_range):\n",
    "    shap_values_dict = {id_unique: [] for id_unique in df_test['ID-unique'].unique()}\n",
    "    \n",
    "    for i in range(20):\n",
    "        for k in range(10):\n",
    "\n",
    "            # Train\n",
    "            ID_train_fold = results_per_fold_train[i][k]['ID-unique']\n",
    "            df_train_fold = df_train[df_train['ID-unique'].isin(ID_train_fold)]\n",
    "            X_train_kf = df_train_fold.iloc[:, feature_col_range]  # Features\n",
    "            y_train_kf = df_train_fold.iloc[:, -2]  # Labels\n",
    "\n",
    "            # Test\n",
    "            ID_test_fold = results_per_fold_test[i][k]['ID-unique']\n",
    "            df_test_fold = df_test[df_test['ID-unique'].isin(ID_test_fold)]\n",
    "            X_test_kf = df_test_fold.iloc[:, feature_col_range]  # Features\n",
    "            y_test_kf = df_test_fold.iloc[:, -2]  # Labels\n",
    "\n",
    "            # Scaling\n",
    "            mean_X_train_kf = results_model['mean_X_train_kf'][k + 10 * i]\n",
    "            std_X_train_kf = results_model['std_X_train_kf'][k + 10 * i]\n",
    "\n",
    "            X_train_kf_scaled = (X_train_kf - mean_X_train_kf) / std_X_train_kf\n",
    "            X_test_kf_scaled = (X_test_kf - mean_X_train_kf) / std_X_train_kf\n",
    "\n",
    "            model_ = results_model['model'][k + 10 * i]\n",
    "\n",
    "            # SHAP calculation\n",
    "            explainer = shap.Explainer(model_, X_train_kf_scaled)\n",
    "            shap_values = explainer.shap_values(X_test_kf_scaled)\n",
    "\n",
    "            # Store SHAP values\n",
    "            for idx, id_unique in enumerate(df_test_fold['ID-unique']):\n",
    "                shap_values_dict[id_unique].append(shap_values[idx])\n",
    "\n",
    "    # Average SHAP values\n",
    "    shap_values_avg_dict = {id_unique: np.mean(values, axis=0) for id_unique, values in shap_values_dict.items()}\n",
    "\n",
    "    # Prepare SHAP summary matrix\n",
    "    shap_values_avg_matrix = [shap_values_avg_dict[id_unique] for id_unique in df_test['ID-unique'].unique()]\n",
    "    shap_values_avg_array = np.array(shap_values_avg_matrix)\n",
    "\n",
    "    feature_names = X_test_kf_scaled.columns.tolist()\n",
    "\n",
    "    shap_values_df = pd.DataFrame(shap_values_avg_array, columns=feature_names)\n",
    "    shap_values_df['ID-unique'] = df_test['ID-unique'].unique()\n",
    "    shap_values_df.set_index('ID-unique', inplace=True)\n",
    "\n",
    "    # SHAP summary\n",
    "    shap_sum = np.abs(shap_values_avg_array).sum(axis=0)\n",
    "    shap_summary = {feature: shap_sum[i] for i, feature in enumerate(feature_names)}\n",
    "    shap_summary_sorted = sorted(shap_summary.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return shap_values_avg_array, shap_summary_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_CN, shap_summary_sorted_CN = shap_values_calculate(\n",
    "    df_concatenado_CN, df_concatenado_CN, results_per_fold_CN_train, results_per_fold_CN_test, results_model, slice(None, -2)\n",
    ")\n",
    "\n",
    "shap_values_AD, shap_summary_sorted_AD = shap_values_calculate(\n",
    "    df_concatenado_CN, df_concatenado_AD, results_per_fold_CN_train, results_per_fold_pat[0], results_model, slice(None, -2)\n",
    ")\n",
    "\n",
    "shap_values_FTD, shap_summary_sorted_FTD = shap_values_calculate(\n",
    "    df_concatenado_FTD, df_concatenado_FTD, results_per_fold_CN_train, results_per_fold_pat[1], results_model, slice(None, -2)\n",
    ")\n",
    "\n",
    "shap_values_MCI, shap_summary_sorted_MCI = shap_values_calculate(\n",
    "    df_concatenado_MCI, df_concatenado_MCI, results_per_fold_CN_train, results_per_fold_pat[2], results_model, slice(None, -2)\n",
    ")\n",
    "\n",
    "shap_values_PD, shap_summary_sorted_PD = shap_values_calculate(\n",
    "    df_concatenado_PD, df_concatenado_PD, results_per_fold_CN_train, results_per_fold_pat[3], results_model, slice(None, -2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_model/shap_summary_sorted_CN.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_summary_sorted_CN, f)\n",
    "\n",
    "with open('results_model/shap_values_CN.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values_CN, f)\n",
    "\n",
    "with open('results_model/shap_summary_sorted_AD.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_summary_sorted_AD, f)\n",
    "\n",
    "with open('results_model/shap_values_AD.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values_AD, f)\n",
    "\n",
    "with open('results_model/shap_summary_sorted_FTD.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_summary_sorted_FTD, f)\n",
    "\n",
    "with open('results_model/shap_values_FTD.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values_FTD, f)\n",
    "\n",
    "with open('results_model/shap_summary_sorted_MCI.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_summary_sorted_MCI, f)\n",
    "\n",
    "with open('results_model/shap_values_MCI.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values_MCI, f)\n",
    "\n",
    "with open('results_model/shap_summary_sorted_PD.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_summary_sorted_PD, f)\n",
    "\n",
    "with open('results_model/shap_values_PD.pkl', 'wb') as f:\n",
    "    pickle.dump(shap_values_PD, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Leer los archivos y cargar los datos en las mismas variables\n",
    "with open('results_model/shap_summary_sorted_CN.pkl', 'rb') as f:\n",
    "    shap_summary_sorted_CN = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_values_CN.pkl', 'rb') as f:\n",
    "    shap_values_CN = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_summary_sorted_AD.pkl', 'rb') as f:\n",
    "    shap_summary_sorted_AD = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_values_AD.pkl', 'rb') as f:\n",
    "    shap_values_AD = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_summary_sorted_FTD.pkl', 'rb') as f:\n",
    "    shap_summary_sorted_FTD = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_values_FTD.pkl', 'rb') as f:\n",
    "    shap_values_FTD = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_summary_sorted_MCI.pkl', 'rb') as f:\n",
    "    shap_summary_sorted_MCI = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_values_MCI.pkl', 'rb') as f:\n",
    "    shap_values_MCI = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_summary_sorted_PD.pkl', 'rb') as f:\n",
    "    shap_summary_sorted_PD = pickle.load(f)\n",
    "\n",
    "with open('results_model/shap_values_PD.pkl', 'rb') as f:\n",
    "    shap_values_PD = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_CN_combined = pd.read_excel('results/df_CN_combined.xlsx')\n",
    "df_AD_combined = pd.read_excel('results/df_AD_combined.xlsx')\n",
    "df_FTD_combined = pd.read_excel('results/df_FTD_combined.xlsx')\n",
    "df_MCI_combined = pd.read_excel('results/df_MCI_combined.xlsx')\n",
    "df_PD_combined = pd.read_excel('results/df_PD_combined.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filtrar el dataframe para incluir solo Educ < 30\n",
    "df_filtered = df_CN_combined[df_CN_combined['Educ'] < 30]\n",
    "Plotters.plot_regresion(y=df_filtered['GAP_corrected'],x= df_filtered['Educ'], label_='Test Data',x_ticks_step=5,y_ticks_step=1,x_min_limit=0,y_min_limit=-3,x_max_limit=30,y_max_limit=3,color='Black', title='GAP vs. Educ ',line_ideal=False,confidence_interval=True,  alpha=0.7,xlabel='Educ',ylabel='GAP',color_confidence_interval='blue',alpha_confidence_interval=0.4,color_line_fit='red', details=False, font='Times New Roman', weight='normal', x_size=3,y_size=3, legend=False, fontsize=13, mode=2, legend_metrics=False, print_metrics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filtrar el dataframe para incluir solo GINI < 30 y eliminar filas con valores nulos en GINI\n",
    "df_filtered = df_CN_combined[(df_CN_combined['Educ'] < 30) & (df_CN_combined['GINI'].notna())]\n",
    "Plotters.plot_regresion(y=df_filtered['GAP_corrected'],x= df_filtered['GINI'], label_='Test Data',x_ticks_step=5,y_ticks_step=1,x_min_limit=25,y_min_limit=-3,x_max_limit=60,y_max_limit=3,color='Black', title='GAP vs. GINI ',line_ideal=False,confidence_interval=True,  alpha=0.7,xlabel='GINI',ylabel='GAP',color_confidence_interval='blue',alpha_confidence_interval=0.4,color_line_fit='red', details=False, font='Times New Roman', weight='normal', x_size=3,y_size=3, legend=False, fontsize=13, mode=2, legend_metrics=False, print_metrics=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
